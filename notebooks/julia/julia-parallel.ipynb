{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel and Distributed Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid gray; padding:10px; width: 95%;\">\n",
    "\n",
    "ðŸ’¡ **Distributed Parallelism vs Shared-Memory Parallelism**\n",
    "\n",
    "- **Distributed Parallelism**: A computing paradigm where a collection of independent computers (nodes), typically interconnected through a network, work together on a task. Each node operates using its own local memory and communicates with other nodes to achieve a common goal.\n",
    "\n",
    "- **Shared-Memory Parallelism**: A computing model where multiple processors (cores) within a single machine access a common shared memory space, allowing for high-speed data exchange and coordination between the processors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared-Memory Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of the Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia's standard library (and underlying libraries like OpenBLAS or MKL) is already optimized to take advantage of multiple cores for many operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Parallel matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000Ã—10000 Matrix{Float64}:\n",
       " 0.0880308  0.462746   0.0349101   â€¦  0.814143  0.711997  0.362411\n",
       " 0.551543   0.991685   0.478472       0.876353  0.143858  0.163324\n",
       " 0.682887   0.728967   0.0768402      0.1669    0.31213   0.672503\n",
       " 0.907196   0.114813   0.882163       0.576486  0.147196  0.307042\n",
       " 0.545289   0.854711   0.266293       0.389206  0.540322  0.921728\n",
       " 0.541738   0.745926   0.00142832  â€¦  0.207691  0.265996  0.71208\n",
       " 0.942256   0.486721   0.439421       0.363547  0.800713  0.0898456\n",
       " 0.0213028  0.846825   0.380308       0.651782  0.231789  0.377306\n",
       " 0.0188436  0.264061   0.14944        0.915886  0.944509  0.239387\n",
       " 0.546724   0.272917   0.67564        0.497031  0.101275  0.321173\n",
       " â‹®                                 â‹±                      \n",
       " 0.0209981  0.362316   0.760614       0.879661  0.690113  0.058412\n",
       " 0.873728   0.0935441  0.211232       0.88765   0.776647  0.0422653\n",
       " 0.74894    0.681661   0.149213       0.240969  0.602014  0.478466\n",
       " 0.645563   0.544908   0.298986       0.296375  0.127909  0.461839\n",
       " 0.203329   0.981661   0.238412    â€¦  0.525408  0.911027  0.0104975\n",
       " 0.87311    0.871884   0.194038       0.301455  0.863251  0.476595\n",
       " 0.188497   0.351533   0.157129       0.486581  0.234802  0.172918\n",
       " 0.224908   0.110131   0.127968       0.306059  0.346634  0.524265\n",
       " 0.226747   0.369327   0.867956       0.503996  0.730333  0.32014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create two large random matrices\n",
    "A = rand(10000, 10000)\n",
    "B = rand(10000, 10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This multiplication will run in parallel on all available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000Ã—10000 Matrix{Float64}:\n",
       " 2515.78  2495.12  2501.34  2503.07  â€¦  2505.85  2467.27  2508.26  2482.7\n",
       " 2505.42  2478.15  2483.36  2499.53     2493.8   2461.16  2509.98  2471.6\n",
       " 2491.55  2492.19  2493.18  2491.78     2508.48  2487.95  2510.86  2485.45\n",
       " 2505.19  2495.36  2513.08  2491.19     2519.35  2490.09  2528.75  2493.04\n",
       " 2530.33  2518.57  2512.8   2516.23     2523.52  2509.21  2541.35  2506.34\n",
       " 2479.6   2493.09  2482.24  2483.6   â€¦  2492.3   2465.81  2508.81  2470.98\n",
       " 2526.61  2509.97  2537.84  2507.06     2519.31  2505.89  2545.45  2507.47\n",
       " 2527.82  2504.72  2526.28  2500.57     2525.36  2497.77  2536.79  2505.71\n",
       " 2513.64  2513.99  2502.87  2491.04     2511.22  2497.12  2526.45  2498.72\n",
       " 2512.05  2527.01  2508.93  2490.9      2510.17  2493.03  2525.36  2494.2\n",
       "    â‹®                                â‹±                             \n",
       " 2555.22  2547.32  2556.49  2540.53     2550.73  2531.81  2568.95  2534.09\n",
       " 2487.69  2480.9   2464.52  2458.69     2498.29  2466.57  2494.53  2483.58\n",
       " 2509.37  2499.6   2512.61  2493.92     2515.75  2480.18  2526.52  2503.24\n",
       " 2532.0   2493.91  2526.47  2507.97     2527.36  2496.46  2527.86  2507.38\n",
       " 2521.57  2510.26  2517.06  2520.76  â€¦  2532.59  2498.97  2523.78  2505.56\n",
       " 2511.92  2503.5   2516.79  2501.5      2514.98  2497.48  2532.39  2510.14\n",
       " 2491.77  2498.09  2485.41  2500.6      2505.0   2497.34  2528.9   2484.71\n",
       " 2499.18  2507.53  2523.14  2513.95     2518.46  2487.72  2541.43  2504.55\n",
       " 2496.89  2503.05  2494.51  2485.39     2500.05  2470.71  2500.78  2478.73"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C = A * B  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Distributed` package in Julia provides functionality for parallel and distributed computing, including:\n",
    "\n",
    "- Management of worker processes.\n",
    "- Remote execution of functions.\n",
    "- Inter-process communication.\n",
    "- Parallel execution of loops and tasks.\n",
    "- Data movement and aggregation across workers.\n",
    "- Asynchronous programming support.\n",
    "- Error handling in a distributed environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Documents/Work/Training/point8/data-science-learning-paths/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Documents/Work/Training/point8/data-science-learning-paths/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"Distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Embarrassingly Parallel Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid gray; padding:10px; width: 95%;\">\n",
    "\n",
    "\n",
    "ðŸ’¡ **Estimating $\\pi$ via Monte Carlo approximation**\n",
    "\n",
    "Curious why this works? Read more on [how to calculate $\\pi$ via Monte Carlo approximation](https://curiosity-driven.org/pi-approximation))\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_pi (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function calculate_pi(n)\n",
    "    inside = 0\n",
    "    for i = 1:n\n",
    "        x = rand()\n",
    "        y = rand()\n",
    "        inside += (x^2 + y^2) <= 1.0 ? 1 : 0\n",
    "    end\n",
    "    return 4 * inside / n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1415852016"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculate_pi(1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Distributed\n",
    "\n",
    "# Add worker processes equal to the number of available cores\n",
    "addprocs(Sys.CPU_THREADS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The @everywhere macro in Julia is used to execute a command on all available worker processes in a distributed computing environment. When you're working with multiple processes (for example, in parallel computing tasks), the @everywhere macro ensures that the enclosed expression is evaluated on each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from process 1\n",
      "      From worker 4:\tHello from process 4\n",
      "      From worker 3:\tHello from process 3\n",
      "      From worker 2:\tHello from process 2\n",
      "      From worker 5:\tHello from process 5\n"
     ]
    }
   ],
   "source": [
    "@everywhere println(\"Hello from process $(myid())\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefixing a function definition with @everywhere is done to define the function across all worker processes in a distributed computing environment. Each process has its own separate workspace and does not automatically have access to the functions and variables defined in the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere begin\n",
    "    \"\"\"\n",
    "        count_inside(n::Int)\n",
    "\n",
    "    Count the number of points that fall inside the unit circle by generating `n` random points.\n",
    "    A point (x, y) is inside the unit circle if x^2 + y^2 <= 1.\n",
    "\n",
    "    # Arguments\n",
    "    - `n::Int`: The number of random points to generate.\n",
    "\n",
    "    # Returns\n",
    "    - `Int`: The count of points that fall inside the unit circle.\n",
    "    \"\"\"\n",
    "    function count_inside(n::Int)\n",
    "        inside = 0\n",
    "        for i = 1:n\n",
    "            x = rand()\n",
    "            y = rand()\n",
    "            inside += (x^2 + y^2) <= 1.0 ? 1 : 0\n",
    "        end\n",
    "        return inside\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_pi_parallel"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    calculate_pi_parallel(total_points::Int)\n",
    "\n",
    "Calculate an estimate of Ï€ using the Monte Carlo method, in parallel.\n",
    "\n",
    "The function distributes the task of generating random points and checking whether they fall\n",
    "inside the quarter of a unit circle across multiple worker processes. It then collects the\n",
    "results from all workers to calculate the final estimate of Ï€.\n",
    "\n",
    "# Arguments\n",
    "- `total_points::Int`: The total number of random points to use for the estimation.\n",
    "\n",
    "# Returns\n",
    "- `Float64`: An estimate of Ï€.\n",
    "\"\"\"\n",
    "function calculate_pi_parallel(total_points::Int)\n",
    "    # Split the work across the workers\n",
    "    points_per_worker = div(total_points, nworkers())\n",
    "    remaining_points = total_points % nworkers()\n",
    "    \n",
    "    # Use @distributed for parallel reduction, summing up the results from each worker\n",
    "    inside_total = @distributed (+) for i = 1:nworkers()\n",
    "        # Handle any remaining points in the last worker\n",
    "        if i == nworkers()\n",
    "            count_inside(points_per_worker + remaining_points)\n",
    "        else\n",
    "            count_inside(points_per_worker)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Calculate pi using the aggregated result\n",
    "    return 4 * inside_total / total_points\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate of Ï€: 3.1415767032\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = Int(1e10)  \n",
    "pi_estimate = calculate_pi_parallel(n)\n",
    "\n",
    "println(\"Estimate of Ï€: $pi_estimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2024 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
