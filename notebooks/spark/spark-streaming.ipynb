{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data Streams with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter introduces the **Spark Structured Streaming API** - a convenient way to process streaming data without having to reason about the details of streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Streaming Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All examples we have seen so far have dealt with processing bounded data sets. In contrast, a **data stream** is an _unbounded sequence of data arriving continuously_. The following table illustrates the differences to the _batch processing_ we have handled so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Â |Â Batch processing |\tStream processingÂ |\n",
    "|---|---|--|\n",
    "|Â **Data scope** |Â \tQueries or processing over all or most of the data in the dataset.|\tQueries or processing over data within a rolling time window, or on just the most recent data record. |\n",
    "|**Data size**\t| Large batches of data.|Â Individual records or micro batches consisting of a few records. |Â \n",
    "|Â **Performance** |Â \tLatencies in minutes to hours.Â |\tRequires latency in the order of seconds or milliseconds. |Â \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Spark Structured Streaming**](https://spark.apache.org/docs/2.3.0/structured-streaming-programming-guide.html) is a scalable stream processing engine built on top of [**ðŸ““ Spark SQL**](spark-structured-data.ipynb). Its main advantage is that stream processing applications can be written in a familiar, declarative way,  without having to reason about the minutiae of streaming. Internally Structured Streaming treats data streams as a series of small batch jobs. Processing each batch is handled efficiently by the Spark SQL engine.\n",
    "\n",
    "![](graphics/third-party/spark-structured-streaming.png)\n",
    "_Source: https://databricks.com_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Example: Streaming Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example, we are going to build another word count application - with an important difference: The text input is not read from a file. It is sent through the network, arriving at a _TCP socket_ - a network endpoint defined by an IP address and a port. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We start by writing th#e stream processing job as a script for PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file scripts/streaming-word-count.py\n",
    "\n",
    "import pyspark\n",
    "# functions for word count\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "\n",
    "# create a Spark SQL session\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StreamingWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# avoid lengthy log messages in output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# define streaming data source\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# define data transformation and output\n",
    "streamingWordCount = lines \\\n",
    "    .select(    \n",
    "       explode(\n",
    "           split(lines.value, \" \")\n",
    "       ).alias(\"word\")\n",
    "    )\\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .sort(col(\"count\").desc()) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()  \n",
    "\n",
    "streamingWordCount.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we discuss the code in detail, let's run it and demonstrate the behavior.\n",
    "\n",
    "2. Before starting the Spark streaming job, the network connection needs to be opened over which streaming input can be sent. We supply input to this connection via the `netcat` command line utility.\n",
    "      ```\n",
    "      > nc -lk 9999\n",
    "      ```\n",
    "3. We start the streaming data processing job in the familiar way using `spark-submit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit scripts/streaming-word-count.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a more detailed look at the code blocks that make up the stream processing job.\n",
    "\n",
    "1. This call defines and connects to a data stream source via a _TCP socket_ - defined by a hostname and a port.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # do not run this for now\n",
    "    lines = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"socket\") \\\n",
    "        .option(\"host\", \"localhost\") \\\n",
    "        .option(\"port\", 9999) \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. This block contains the word count data processing step using familiar `pyspark.sql.DataFrame` operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # do not run this for now\n",
    "    streamingWordCount = lines \\\n",
    "        .select(    \n",
    "           explode(\n",
    "               split(lines.value, \" \")\n",
    "           ).alias(\"word\")\n",
    "        )\\\n",
    "        .groupBy(\"word\") \\\n",
    "        .count() \\\n",
    "        .sort(col(\"count\").desc()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Finally, we direct streaming output (generated whenever new incoming data arrives) to the terminal and start the processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    streamingWordCount \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Ended Project: Twitter Stream Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the Twitter API to read and analyze a stream of tweets associated with a specific hashtag in real time**\n",
    "\n",
    "Prerequisites:\n",
    "- a Twitter developer account to generate API keys\n",
    "\n",
    "Useful Python libraries:\n",
    "- `getpass`\n",
    "- [`tweepy`](https://www.tweepy.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [What is Streaming Data?](https://aws.amazon.com/streaming-data/)\n",
    "- [Structured Streaming Programming Guide](https://spark.apache.org/docs/2.3.0/structured-streaming-programming-guide.html)\n",
    "- [Streaming + Scikit-Learn](https://towardsdatascience.com/streaming-scikit-learn-with-pyspark-c4806116a453)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
